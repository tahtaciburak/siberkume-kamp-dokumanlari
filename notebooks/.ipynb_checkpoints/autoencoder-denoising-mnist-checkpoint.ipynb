{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Generate corrupted MNIST images by adding noise with normal dist\n",
    "# centered at 0.5 and std=0.5\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
    "x_train_noisy = x_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
    "x_test_noisy = x_test + noise\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "# Network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "latent_dim = 16\n",
    "# Encoder/Decoder number of CNN layers and filters per layer\n",
    "layer_filters = [32, 64]\n",
    "\n",
    "# Build the Autoencoder Model\n",
    "# First build the Encoder Model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latent_dim, name='latent_vector')(x)\n",
    "\n",
    "# Instantiate Encoder Model\n",
    "encoder = Model(inputs, latent, name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Build the Decoder Model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train_noisy,\n",
    "                x_train,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                epochs=30,\n",
    "                batch_size=batch_size)\n",
    "\n",
    "# Predict the Autoencoder output from corrupted test images\n",
    "x_decoded = autoencoder.predict(x_test_noisy)\n",
    "\n",
    "# Display the 1st 8 corrupted and denoised images\n",
    "rows, cols = 10, 30\n",
    "num = rows * cols\n",
    "imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n",
    "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
    "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
    "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "imgs = (imgs * 255).astype(np.uint8)\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Original images: top rows, '\n",
    "          'Corrupted Input: middle rows, '\n",
    "          'Denoised Input:  third rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
